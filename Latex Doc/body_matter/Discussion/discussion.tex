\chapter{\tl{Discussion} }
\en{

% Overall stuff:
Looking at the results, we can clearly see that the models that achieve superior results are the OPNMF and MCA models. The first model, employed the OPNMF technique in order to transform the 145 ROI values for the imaging view of the dataset into 30 imaging components. After that, without the genetic view, balancing on the imaging view was performed, and the classification was performed after training a Bagging ensemble classifier, using as a base model a linear SVM, achieving the highest balanced accuracy score, of 61.01\%. The second model, utilized the MCA technique on the genetic view of the dataset, reducing the 54 (categorical) values of the SNPs into 10 genetic components (continuous). After that, along with the imaging view, balancing was performed, and the classification task was carried out using the model of SVM (single classifier). The polynomial kernel as well as the radial basis function were both used and both achieved identical results, which are the best in accuracy and F1 Score, 59.59\% and 57.85\% respectively. 

% Genetic only is not good:
It is immediately obvious that all models are achieving scores that are close to each other, with their best variations being around 50\% and above. That however is not the case for most variations of the models' parameters, as is easily observable through the figures of the fifth chapter (e.g. \ref{fig: Summary Table for classification scores for raw data},\ref{fig: Summary Table for classification scores for DCCA data},\ref{fig: Summary Table for classification scores for MCA transformed data},\ref{fig: Summary Table for classification scores for MCA - DCCA transformed data}). Therefore, there is a clear benefit in using the imaging view, since many genetic-only models are quite far from the best models (achieving metric scores in the mid 30s). More importantly, no genetic-only model has been able to achieve better results than another model that included the imaging view. This is expected, since the genetic data do not contain any information about MCI patients, and more importantly, the existence of AD risk alleles are not guaranteed to be translated to having the disease. Furthermore, the information that is contained in the neuroimaging data is very important, and many times is a better indicator than the genetic data. Studies utilizing genetic only data have achieved prediction results that are worse than the results of models that used imaging data and/or genetic data (\cite{83}, \cite{84}, \cite{85}). This is also evaluated by studies that have compared using the two views, and found that using only genetic data is relatively worse \cite{86}

% Imaging only is not always the answer:
Nevertheless, the same cannot be said for the opposite; that is there is no clear benefit in including always using both the genetic and the imaging views, since models that have both views (imaging and genetic) are often outperformed by imaging-only models, yet they trade places depending on the model. For example, the two previously mentioned models (that achieve the best metric scores) are imaging-only (the case of the OPNMF model), and both views (the case of the MCA model). 

% Balancing and scaling have unclear effects:
There is no clear conclusion to be drawn about the benefit of the techniques of balancing and scaling. In the cases of (a) the data having no data analysis technique applied to them before classification, i.e. raw, (b) MCA-transformed genetic view models, (c)  MCA-transformed genetic view and OPNMF-transformed imaging view models, balancing the dataset and applying scaling if needed seems to help, while in the case of the rest of the models it seems to worsen the results. This might be because information could be perhaps lost due to the method of standard scaling.

% DCCA is worse:
As for DCCA, it is clear that the technique achieves its target, which is to increase the linear correlation between the two views. However, one can clearly observe that it produces objectively worse results than equivalent no-DCCA methods. Even in the case of the original data, the DCCA transformed data produce results that are across the board worse in every metric. This is compounded by the fact that the DCCA networks not only took a considerable amount of time to train and optimize, but also the transformed outputs were clearly more computationally expensive to classify and therefore the fitting of the classifiers to the DCCA-transformed dataset took more time than that of their no-DCCA counterpart methods. The aforementioned facts can only lead us to conclude that the use of the method cannot be recommended for this type of problem, with the reservation that the model was not adequately trained or had not a nearly enough number of parameters. This however could very well go beyond the scope of this work, and is discussed in the next chapter. One detail that should be noted is that the DCCA method is not suitable for the great dissimilarity of the two views (the imaging being continuous and the genetic view being categorical). We attempted to remedy that dissimilarity with the MCA and OPNMF methods, which definitely improved the results of the models that utilized the DCCA method, however none of the resulting methods could achieve the results that the no-DCCA equivalent methods could achieve. While the model has the ability to non-linearly transform data in such a way that they are maximally correlated, this case is not a suitable case, as proven by the results. One reason might be that neural networks often require a much greater volume of data, and another might be that the two views are different in nature. 

% What about FAMD?
An interesting result is that of FAMD, in which both views are transformed into a reduced number of components. This method, as previously mentioned, is similar to performing PCA on the continuous imaging view while performing MCA on the categorical genetic view, but combined. In our case, we set the number of components to 10 due to long computational time, which proved to yield worse results than many of the other methods. It is entirely possible that the number of chosen components might not be enough to capture all of the information that the two views provide, and only achieve a substantial dimensionality reduction.

% Ensembles are better than single classifiers: 
As can be seen by the extensive figures of the previous chapters, or more conveniently from the table's \ref{fig:Summary table} notes, the ensemble classifiers were in general more successful in predicting the class. This aligns well with the empirical knowledge that ensemble methods of even simple (as decision trees and linear SVMs are) can improve upon the single method performance, even if the single method is more complex. The performance of the ensemble classifiers achieved not only better best-case results, but on average was better overall, with even the worst performing parameter combinations beating out the worst performing parameter combinations of the single classifier models of SVM. This was intuitively hypothesized, emprically proven, as well as expected as it has been shown in other studies. (\cite{87}, \cite{88})

% Ensemble methods comparison:
Moreover, comparing the different ensemble methods, it is not clear as to which method has the clear edge. The two methods have comparable results, with Bagging seemingly being in more models marginally better than AdaBoost. Furthermore, comparing the two methods for the base model classifier, which are the decision tree model and the linear kernel SVM, we can conclude that in nearly every case, the linear SVM base model is better, for both kinds of ensembles. While the difference between the performances of the two base models is not substantial, there is a clear trend. It is worth noting that both of those base models were chosen for their relatively simplistic nature, because of computational time limitations and as to create a basis for comparing single classifier models to ensemble classifiers.  

% SVM kernel comparison:
Overall, ensemble methods result in higher classification performance compared to single classifier models. Concerning the single classifier model of the different SVM kernels, we can see that in most cases the polynomial and RBF kernels outperform the linear kernel. Additionally, the polynomial and RBF kernels seem to outperform in the best case the linear kernel, however the linear kernel seems to be more robust. That can be seen in many confusion matrices, where the polynomial and RBF kernels classify the dataset with very poor performance. There is however, a very interesting note to be added, concerning the models that had DCCA applied to the dataset before the classification task. In those cases, the linear kernel is better than the polynomial and RBF kernels. This might indicate that indeed the DCCA method achieves the goal of linearly correlating the two views, however that might be at the expense of information loss, as those models are beaten by simpler methods without having DCCA applied to the dataset. This alone might be indicative of the need for not only more computational time and power devoted to the training of the DCCA parallel neural networks, but also the need for bigger and more complex variations of the network explored. In agreement with the original paper, after we applied DCCA, the transformed views were more correlated than the original ones, so we succeeded at the goal of the DCCA method.

% Inferring from the Confusion Matrices:
Another important aspect that can be observed is the success of the models in the classification of a specific class. We can see that most models (except from the models with distinctively poor overall performance) could reliably classify the patients that were suffering from AD from the CN people, while struggling to accurately decide for the case of the MCI class. The confusion matrices in the figures of the previous chapter highlight exactly that, with most models having a high percentage of the upper left portion of their confusion matrix well-defined, while the outer right and bottom part (which is the part of the MCI class) being quite confounded. As an example, one can observe the confusion matrices for the OPNMF method (no-DCCA), with the classifier being an ensemble classifier especially on using both or only the imaging view (\ref{fig:OPNMF Bagging Confusion Matrices with scaling and balancing} Bagging, \ref{fig:OPNMF AdaBoost Confusion Matrices with scaling and balancing} Adaboost), but also the confusion matrices of the MCA method on both views, with scaling or balancing, using a single classifier model (\ref{fig:Confusion Matrices of MCA vs MCA-DCCA data with scaling and balancing} SVMs). This is indicative of the strong performance of the model in AD vs CN classification, as well as the difficulty of the problem to distinguish CN vs MCI and MCI vs AD, as the MCI class is apparently blurring the lines of the other classes. Since the methods and the models were optimized and trained for the problem of multi-class classification on the classes of CN, MCI and AD aforementioned should not be taken as a fact, but rather as an indication of performance. 

% Limitations:
The utilization of the grid search and cross validation methods for the evaluation of the different model parameters' performance while producing extensive and very useful results, was limiting the number of models that could possibly be explored. Both the optimizations of the DCCA networks, as well as the classification methods training, took for every model time in the order of hours. As a result, only relatively simple methods such as Support Vector Machines and Decision Trees were selected to be explored, since training multilayer perceptrons and similar methdos for each classification task and then optimizing them not only with every model and every view, but also with their respective grids of parameters was out of the question. Another limiting factor was the insufficient amount of data points that are inherently available to studies like these, which is due to the nature of the problem, since biomedical data and especially neuroimaging data are not only hard to collect, but pose storing, processing and visualization problems as well. Finally, the time frame for this study was not unbounded, and thus there had to be a selective process as to the direction of experimentation. 

% Future Directions:
Finally, as to what the future directions might be for further research on the lessons learned from this work, there are many possible steps. One might be to explore, as mentioned previously, different optimization algorithms, activation functions and generally different architectures for the parallel neural networks of the DCCA method. Another direction might be data augmentation, or creation of synthetic data, as to enrich the dataset and unlock the potential of the DCCA method. Concerning the data analysis techniques, the OPNMF method is one that could benefit from experimenting with different number of components, something that wasn't done on this study due to computational power limitations. Furthermore, more complex classifiers can be used, such as MLPs and K-Nearest Neighbors classifiers. That can also be extended to the ensemble methods, not only for the base classifier models, but also for the ensemble methods, with other boosting methods and methods such as stacking being obvious candidates. Finally, to address the issue of the different in nature and type views, one could explore the use of deep autoencoders in order to alleviate the problem of handling categorical data. }