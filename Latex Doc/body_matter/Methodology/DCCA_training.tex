\section{\tl{DCCA model training}} \label{DCCA methodology}
\en{In addition to the previously mentioned techniques, DCCA was used in order to transform the two uncorrelated, different views into two that are more linearly correlated. The choice of hyperparameters, the methods and class functions, as well as the parameter optimization was subject to our experimentation. 

As stated before, the DCCA model relies on two parallel networks, each taking as input a view of the dataset, and producing the output that are the views but nonlinearly transformed in order to use as subject for classification. 

The dataset for the model optimization was split into three sets, the training, validation and the test set, with the last one being kept hidden from the model during the training phase, in order to ensure an accurate prediction on unseen data. The split between the sets was 75\% for the training set, 15\% for the validation set, and 10\% for the test set, split randomly. This was done in order to ensure that the model had enough training samples, as well as validation samples to achieve good accuracy scores without overfitting, and enough test scores as to not skew the results. 

The hyperparameters include the number of hidden layers as well as hidden layer size, output layer size, the regularization parameter, the learning rate, as well as the batch size. These hyperparameters were chosen after extensive testing with each one, with the best values stored and used for training. The epoch number was kept at 100 epochs along all experimentation, which was enough for all cases for the validation accuracy score to stabilize. 

We experimented with 3 or 4 hidden layers, as it became apparent that due to the complexity of the problem, a big enough network was needed for both views. The original paper used the same architecture for both of the networks, a logic we followed in our study as well. The size of the hidden layers ranged from 256 neurons to 1024 neurons per layer, with all hidden layers having the same number of neurons. As for the output layer size, we experimented with sizes of [10,50,100,150]. One attempt was made with output layer size of 300 in order to observe the effect of large output layer size in the correlation metric, however the computation was extremely time-consuming. Furthermore, the learning rate ranged from $10^{-4}$ to $10^{-2}$, and the regularization parameter being in the range of $10^{-4}$ to $10^{-2}$. The batch size that we used was either 500 samples or 1000 samples. 

The activation function was kept the same as the original paper, which was a sigmoid function, and the error metric was the error metric as defined from the paper, a version of CCA using a derivative-free optimization method. The optimizer function of the paper was the L-BFGS second-order optimization method, however due to it being more difficult to compute, the optimizer RMSProp was used, with similar results. The original paper initialized the neural network using a denoising autoencoder, but this was out of the scope of this study.  \cite{76}

The DCCA implementation we used was made with python by Z.Wu. The method is implemented with pytorch, which supports for multi-GPU training, however this study employed only CPU training.  (\cite{77}, \cite{78})
}