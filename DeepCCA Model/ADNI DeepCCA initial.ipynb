{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97cf239",
   "metadata": {},
   "source": [
    "# Use DeepCCA to transform ADNI features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3582a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from linear_cca import linear_cca\n",
    "from torch.utils.data import BatchSampler, SequentialSampler\n",
    "from DeepCCAModels import DeepCCA\n",
    "from main import Solver\n",
    "from utils import load_data, svm_classify\n",
    "from objectives import cca_loss\n",
    "try:\n",
    "    import cPickle as thepickle\n",
    "except ImportError:\n",
    "    import _pickle as thepickle\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08db7d",
   "metadata": {},
   "source": [
    "## Read the database, examine it:\n",
    "\n",
    "Instead of reading the whole database, we read only the data that's useful to us. That is, we read only specific columns of data, and we take only the row containing the first scan for each person. \n",
    "\n",
    "In \"ADNI Regressional Analysis.ipynb\" we have done that exactly, as well as performed linear regression transformation to the imaging data, in order to remove any age, sex, and DLICV_baseline effect. \n",
    "\n",
    "Furthermore, in \"ADNI OPNMF.ipynb\" we have performed dimensionality reduction through the OPNMF method, reducing the number of the ROIs from 145 to just 18. (Hasn't been done so this does not apply)\n",
    "\n",
    "The data is located at \"./DATA/Reduced_Linearly_Transformed_Unique_Dataset.pkl\" \n",
    "\n",
    "(Need to run the RA code if data is not found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009c840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1302, 209)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTID</th>\n",
       "      <th>MRID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>DLICV_baseline</th>\n",
       "      <th>APOE4_Alleles</th>\n",
       "      <th>APOE_Genotype</th>\n",
       "      <th>Diagnosis_nearest_2.0</th>\n",
       "      <th>MUSE_Volume_4</th>\n",
       "      <th>...</th>\n",
       "      <th>rs111278892</th>\n",
       "      <th>rs3752246</th>\n",
       "      <th>rs4147929</th>\n",
       "      <th>rs41289512</th>\n",
       "      <th>rs3865444</th>\n",
       "      <th>rs6024870</th>\n",
       "      <th>rs6014724</th>\n",
       "      <th>rs7274581</th>\n",
       "      <th>rs429358</th>\n",
       "      <th>Diagnosis_nearest_2.0_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002_S_0295</td>\n",
       "      <td>002_S_0295_2006-04-18</td>\n",
       "      <td>2006-04-18</td>\n",
       "      <td>84.742466</td>\n",
       "      <td>0</td>\n",
       "      <td>1485405.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E3/E4</td>\n",
       "      <td>CN</td>\n",
       "      <td>-401.428503</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>002_S_0413</td>\n",
       "      <td>002_S_0413_2006-05-02</td>\n",
       "      <td>2006-05-02</td>\n",
       "      <td>76.283562</td>\n",
       "      <td>1</td>\n",
       "      <td>1364116.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E3/E3</td>\n",
       "      <td>CN</td>\n",
       "      <td>596.355045</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>002_S_0559</td>\n",
       "      <td>002_S_0559_2006-05-23</td>\n",
       "      <td>2006-05-23</td>\n",
       "      <td>79.223288</td>\n",
       "      <td>0</td>\n",
       "      <td>1570479.625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E3/E4</td>\n",
       "      <td>CN</td>\n",
       "      <td>224.874560</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>002_S_0619</td>\n",
       "      <td>002_S_0619_2006-06-01</td>\n",
       "      <td>2006-06-01</td>\n",
       "      <td>77.447945</td>\n",
       "      <td>0</td>\n",
       "      <td>1859348.250</td>\n",
       "      <td>2.0</td>\n",
       "      <td>E4/E4</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>2633.277779</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>002_S_0729</td>\n",
       "      <td>002_S_0729_2006-07-17</td>\n",
       "      <td>2006-07-17</td>\n",
       "      <td>65.056164</td>\n",
       "      <td>1</td>\n",
       "      <td>1166961.750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E3/E4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>256.289641</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>002_S_0816</td>\n",
       "      <td>002_S_0816_2006-08-30</td>\n",
       "      <td>2006-08-30</td>\n",
       "      <td>70.767123</td>\n",
       "      <td>0</td>\n",
       "      <td>1444128.125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>E4/E4</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>-126.260419</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>002_S_0938</td>\n",
       "      <td>002_S_0938_2006-10-05</td>\n",
       "      <td>2006-10-05</td>\n",
       "      <td>82.167123</td>\n",
       "      <td>1</td>\n",
       "      <td>1309685.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E3/E3</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>200.102369</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>002_S_0954</td>\n",
       "      <td>002_S_0954_2006-10-10</td>\n",
       "      <td>2006-10-10</td>\n",
       "      <td>69.198630</td>\n",
       "      <td>1</td>\n",
       "      <td>1075661.500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E3/E4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>-60.539913</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>002_S_0955</td>\n",
       "      <td>002_S_0955_2006-10-11</td>\n",
       "      <td>2006-10-11</td>\n",
       "      <td>78.161644</td>\n",
       "      <td>1</td>\n",
       "      <td>1363607.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E3/E4</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>1058.028132</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>002_S_1018</td>\n",
       "      <td>002_S_1018_2006-11-29</td>\n",
       "      <td>2006-11-29</td>\n",
       "      <td>70.658904</td>\n",
       "      <td>1</td>\n",
       "      <td>1355603.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E3/E3</td>\n",
       "      <td>Dementia</td>\n",
       "      <td>-485.048304</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>002_S_1070</td>\n",
       "      <td>002_S_1070_2006-11-28</td>\n",
       "      <td>2006-11-28</td>\n",
       "      <td>73.564384</td>\n",
       "      <td>0</td>\n",
       "      <td>1550701.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E3/E3</td>\n",
       "      <td>MCI</td>\n",
       "      <td>266.235891</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>002_S_1261</td>\n",
       "      <td>002_S_1261_2007-02-15</td>\n",
       "      <td>2007-02-15</td>\n",
       "      <td>71.067123</td>\n",
       "      <td>1</td>\n",
       "      <td>1350714.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E3/E3</td>\n",
       "      <td>CN</td>\n",
       "      <td>-202.715174</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>002_S_1268</td>\n",
       "      <td>002_S_1268_2007-02-14</td>\n",
       "      <td>2007-02-14</td>\n",
       "      <td>82.642466</td>\n",
       "      <td>0</td>\n",
       "      <td>1435189.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E3/E4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>246.512659</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>002_S_2043</td>\n",
       "      <td>002_S_2043_2010-08-31</td>\n",
       "      <td>2010-08-31</td>\n",
       "      <td>72.180822</td>\n",
       "      <td>1</td>\n",
       "      <td>1280567.125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E3/E4</td>\n",
       "      <td>MCI</td>\n",
       "      <td>-499.901619</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>002_S_4171</td>\n",
       "      <td>002_S_4171_2011-08-08</td>\n",
       "      <td>2011-08-08</td>\n",
       "      <td>69.353425</td>\n",
       "      <td>0</td>\n",
       "      <td>1522107.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E3/E3</td>\n",
       "      <td>MCI</td>\n",
       "      <td>72.675098</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PTID                   MRID        Date        Age  Sex  \\\n",
       "0    002_S_0295  002_S_0295_2006-04-18  2006-04-18  84.742466    0   \n",
       "9    002_S_0413  002_S_0413_2006-05-02  2006-05-02  76.283562    1   \n",
       "24   002_S_0559  002_S_0559_2006-05-23  2006-05-23  79.223288    0   \n",
       "31   002_S_0619  002_S_0619_2006-06-01  2006-06-01  77.447945    0   \n",
       "45   002_S_0729  002_S_0729_2006-07-17  2006-07-17  65.056164    1   \n",
       "64   002_S_0816  002_S_0816_2006-08-30  2006-08-30  70.767123    0   \n",
       "69   002_S_0938  002_S_0938_2006-10-05  2006-10-05  82.167123    1   \n",
       "74   002_S_0954  002_S_0954_2006-10-10  2006-10-10  69.198630    1   \n",
       "81   002_S_0955  002_S_0955_2006-10-11  2006-10-11  78.161644    1   \n",
       "84   002_S_1018  002_S_1018_2006-11-29  2006-11-29  70.658904    1   \n",
       "90   002_S_1070  002_S_1070_2006-11-28  2006-11-28  73.564384    0   \n",
       "113  002_S_1261  002_S_1261_2007-02-15  2007-02-15  71.067123    1   \n",
       "127  002_S_1268  002_S_1268_2007-02-14  2007-02-14  82.642466    0   \n",
       "147  002_S_2043  002_S_2043_2010-08-31  2010-08-31  72.180822    1   \n",
       "163  002_S_4171  002_S_4171_2011-08-08  2011-08-08  69.353425    0   \n",
       "\n",
       "     DLICV_baseline  APOE4_Alleles APOE_Genotype Diagnosis_nearest_2.0  \\\n",
       "0       1485405.375            1.0         E3/E4                    CN   \n",
       "9       1364116.000            0.0         E3/E3                    CN   \n",
       "24      1570479.625            1.0         E3/E4                    CN   \n",
       "31      1859348.250            2.0         E4/E4              Dementia   \n",
       "45      1166961.750            1.0         E3/E4                   MCI   \n",
       "64      1444128.125            2.0         E4/E4              Dementia   \n",
       "69      1309685.000            0.0         E3/E3              Dementia   \n",
       "74      1075661.500            1.0         E3/E4                   MCI   \n",
       "81      1363607.000            1.0         E3/E4              Dementia   \n",
       "84      1355603.000            0.0         E3/E3              Dementia   \n",
       "90      1550701.375            0.0         E3/E3                   MCI   \n",
       "113     1350714.875            0.0         E3/E3                    CN   \n",
       "127     1435189.875            1.0         E3/E4                   MCI   \n",
       "147     1280567.125            1.0         E3/E4                   MCI   \n",
       "163     1522107.375            0.0         E3/E3                   MCI   \n",
       "\n",
       "     MUSE_Volume_4  ...  rs111278892  rs3752246  rs4147929  rs41289512  \\\n",
       "0      -401.428503  ...            1          1          1           0   \n",
       "9       596.355045  ...            0          1          1           0   \n",
       "24      224.874560  ...            0          0          0           0   \n",
       "31     2633.277779  ...            0          0          0           1   \n",
       "45      256.289641  ...            0          0          0           1   \n",
       "64     -126.260419  ...            0          0          0           0   \n",
       "69      200.102369  ...            0          1          1           0   \n",
       "74      -60.539913  ...            2          1          1           0   \n",
       "81     1058.028132  ...            1          0          0           0   \n",
       "84     -485.048304  ...            1          1          1           0   \n",
       "90      266.235891  ...            0          0          0           0   \n",
       "113    -202.715174  ...            0          0          0           0   \n",
       "127     246.512659  ...            0          0          0           0   \n",
       "147    -499.901619  ...            0          0          0           1   \n",
       "163      72.675098  ...            0          0          0           0   \n",
       "\n",
       "     rs3865444  rs6024870  rs6014724  rs7274581  rs429358  \\\n",
       "0            0          0          0          0         1   \n",
       "9            1          0          0          0         0   \n",
       "24           1          0          0          0         0   \n",
       "31           1          0          0          0         2   \n",
       "45           1          0          0          0         1   \n",
       "64           1          0          0          0         2   \n",
       "69           1          0          0          0         0   \n",
       "74           1          0          0          0         1   \n",
       "81           1          0          0          0         1   \n",
       "84           0          0          0          0         0   \n",
       "90           0          0          0          0         0   \n",
       "113          0          1          1          1         0   \n",
       "127          1          1          1          1         1   \n",
       "147          0          0          0          0         1   \n",
       "163          0          0          0          1         0   \n",
       "\n",
       "     Diagnosis_nearest_2.0_cat  \n",
       "0                            0  \n",
       "9                            0  \n",
       "24                           0  \n",
       "31                           1  \n",
       "45                           2  \n",
       "64                           1  \n",
       "69                           1  \n",
       "74                           2  \n",
       "81                           1  \n",
       "84                           1  \n",
       "90                           2  \n",
       "113                          0  \n",
       "127                          2  \n",
       "147                          2  \n",
       "163                          2  \n",
       "\n",
       "[15 rows x 209 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique = pd.read_pickle(\"./DATA/Linearly_Transformed_Unique_Dataset.pkl\")\n",
    "print(unique.shape)\n",
    "unique.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e714f1-ad2e-4d18-9b55-265cfbac0509",
   "metadata": {},
   "source": [
    "##  Building, training, and producing the new features by DCCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c020f",
   "metadata": {},
   "source": [
    "### Create the 2 views:\n",
    "\n",
    "The first view consists of the imaging data, that are in the form of 145 real numbers. Those numbers are based on a prediction from a Linear Regression estimator trained only on the Cognitive Normal datapoints. The predictions then are subtracted from the actual values, and the remaining value (residual) is the datapoint for each ROI.\n",
    "\n",
    "The second view consists of the 54 SNP (Single Nucleotide Polymorphism, \"snip\"), for each individual. They are either 0 or 1. \n",
    "\n",
    "The 2 views are the most basic views that can be used for the Deep CCA, and in further tests more features will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ded1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 1:\n",
    "view_1 = unique.loc[:,\"MUSE_Volume_4\":\"MUSE_Volume_207\"]\n",
    "\n",
    "# View 2:\n",
    "view_2 = unique.loc[:,\"rs4575098\":\"rs429358\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afced851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MUSE_Volume_4</th>\n",
       "      <th>MUSE_Volume_11</th>\n",
       "      <th>MUSE_Volume_23</th>\n",
       "      <th>MUSE_Volume_30</th>\n",
       "      <th>MUSE_Volume_31</th>\n",
       "      <th>MUSE_Volume_32</th>\n",
       "      <th>MUSE_Volume_35</th>\n",
       "      <th>MUSE_Volume_36</th>\n",
       "      <th>MUSE_Volume_37</th>\n",
       "      <th>MUSE_Volume_38</th>\n",
       "      <th>...</th>\n",
       "      <th>MUSE_Volume_198</th>\n",
       "      <th>MUSE_Volume_199</th>\n",
       "      <th>MUSE_Volume_200</th>\n",
       "      <th>MUSE_Volume_201</th>\n",
       "      <th>MUSE_Volume_202</th>\n",
       "      <th>MUSE_Volume_203</th>\n",
       "      <th>MUSE_Volume_204</th>\n",
       "      <th>MUSE_Volume_205</th>\n",
       "      <th>MUSE_Volume_206</th>\n",
       "      <th>MUSE_Volume_207</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-401.428503</td>\n",
       "      <td>-475.082401</td>\n",
       "      <td>-38.009137</td>\n",
       "      <td>-25.646775</td>\n",
       "      <td>75.669198</td>\n",
       "      <td>126.590353</td>\n",
       "      <td>124.482040</td>\n",
       "      <td>54.813183</td>\n",
       "      <td>29.901574</td>\n",
       "      <td>4088.590752</td>\n",
       "      <td>...</td>\n",
       "      <td>789.359109</td>\n",
       "      <td>-170.422388</td>\n",
       "      <td>-1543.880458</td>\n",
       "      <td>-1674.618225</td>\n",
       "      <td>632.563713</td>\n",
       "      <td>-531.585707</td>\n",
       "      <td>-39.623451</td>\n",
       "      <td>260.649954</td>\n",
       "      <td>-115.085767</td>\n",
       "      <td>-81.954942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>596.355045</td>\n",
       "      <td>-177.499304</td>\n",
       "      <td>34.866428</td>\n",
       "      <td>-42.623326</td>\n",
       "      <td>36.073837</td>\n",
       "      <td>53.399103</td>\n",
       "      <td>-914.581355</td>\n",
       "      <td>-108.931623</td>\n",
       "      <td>124.883198</td>\n",
       "      <td>-972.814733</td>\n",
       "      <td>...</td>\n",
       "      <td>1335.363413</td>\n",
       "      <td>2649.942277</td>\n",
       "      <td>1411.230818</td>\n",
       "      <td>31.346635</td>\n",
       "      <td>-1267.747385</td>\n",
       "      <td>117.940453</td>\n",
       "      <td>-149.481247</td>\n",
       "      <td>-535.823873</td>\n",
       "      <td>-40.431290</td>\n",
       "      <td>-470.993059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>224.874560</td>\n",
       "      <td>1110.220538</td>\n",
       "      <td>134.798531</td>\n",
       "      <td>135.858384</td>\n",
       "      <td>109.271915</td>\n",
       "      <td>29.323465</td>\n",
       "      <td>1704.716715</td>\n",
       "      <td>-284.353318</td>\n",
       "      <td>-15.319536</td>\n",
       "      <td>-1099.592092</td>\n",
       "      <td>...</td>\n",
       "      <td>-1719.736333</td>\n",
       "      <td>-2192.002637</td>\n",
       "      <td>1513.234184</td>\n",
       "      <td>523.058557</td>\n",
       "      <td>1603.211181</td>\n",
       "      <td>2422.347525</td>\n",
       "      <td>218.967561</td>\n",
       "      <td>-66.321636</td>\n",
       "      <td>-200.393450</td>\n",
       "      <td>-107.128517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2633.277779</td>\n",
       "      <td>703.512999</td>\n",
       "      <td>-165.584181</td>\n",
       "      <td>-128.673356</td>\n",
       "      <td>-351.196493</td>\n",
       "      <td>-369.996116</td>\n",
       "      <td>-3669.094187</td>\n",
       "      <td>-871.937556</td>\n",
       "      <td>-752.247876</td>\n",
       "      <td>-503.572274</td>\n",
       "      <td>...</td>\n",
       "      <td>-616.963790</td>\n",
       "      <td>603.136888</td>\n",
       "      <td>-608.460701</td>\n",
       "      <td>-1332.047832</td>\n",
       "      <td>-3012.228200</td>\n",
       "      <td>-2804.914271</td>\n",
       "      <td>-791.597461</td>\n",
       "      <td>-822.562965</td>\n",
       "      <td>-356.240988</td>\n",
       "      <td>-118.945277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>256.289641</td>\n",
       "      <td>599.953746</td>\n",
       "      <td>10.855725</td>\n",
       "      <td>32.230306</td>\n",
       "      <td>-150.404626</td>\n",
       "      <td>-107.055211</td>\n",
       "      <td>472.978958</td>\n",
       "      <td>649.045998</td>\n",
       "      <td>130.401624</td>\n",
       "      <td>-85.434102</td>\n",
       "      <td>...</td>\n",
       "      <td>-317.885924</td>\n",
       "      <td>-670.024556</td>\n",
       "      <td>62.647813</td>\n",
       "      <td>986.709609</td>\n",
       "      <td>-1459.773538</td>\n",
       "      <td>-1367.048968</td>\n",
       "      <td>-141.267965</td>\n",
       "      <td>-197.021311</td>\n",
       "      <td>-217.041629</td>\n",
       "      <td>-277.950585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MUSE_Volume_4  MUSE_Volume_11  MUSE_Volume_23  MUSE_Volume_30  \\\n",
       "0     -401.428503     -475.082401      -38.009137      -25.646775   \n",
       "9      596.355045     -177.499304       34.866428      -42.623326   \n",
       "24     224.874560     1110.220538      134.798531      135.858384   \n",
       "31    2633.277779      703.512999     -165.584181     -128.673356   \n",
       "45     256.289641      599.953746       10.855725       32.230306   \n",
       "\n",
       "    MUSE_Volume_31  MUSE_Volume_32  MUSE_Volume_35  MUSE_Volume_36  \\\n",
       "0        75.669198      126.590353      124.482040       54.813183   \n",
       "9        36.073837       53.399103     -914.581355     -108.931623   \n",
       "24      109.271915       29.323465     1704.716715     -284.353318   \n",
       "31     -351.196493     -369.996116    -3669.094187     -871.937556   \n",
       "45     -150.404626     -107.055211      472.978958      649.045998   \n",
       "\n",
       "    MUSE_Volume_37  MUSE_Volume_38  ...  MUSE_Volume_198  MUSE_Volume_199  \\\n",
       "0        29.901574     4088.590752  ...       789.359109      -170.422388   \n",
       "9       124.883198     -972.814733  ...      1335.363413      2649.942277   \n",
       "24      -15.319536    -1099.592092  ...     -1719.736333     -2192.002637   \n",
       "31     -752.247876     -503.572274  ...      -616.963790       603.136888   \n",
       "45      130.401624      -85.434102  ...      -317.885924      -670.024556   \n",
       "\n",
       "    MUSE_Volume_200  MUSE_Volume_201  MUSE_Volume_202  MUSE_Volume_203  \\\n",
       "0      -1543.880458     -1674.618225       632.563713      -531.585707   \n",
       "9       1411.230818        31.346635     -1267.747385       117.940453   \n",
       "24      1513.234184       523.058557      1603.211181      2422.347525   \n",
       "31      -608.460701     -1332.047832     -3012.228200     -2804.914271   \n",
       "45        62.647813       986.709609     -1459.773538     -1367.048968   \n",
       "\n",
       "    MUSE_Volume_204  MUSE_Volume_205  MUSE_Volume_206  MUSE_Volume_207  \n",
       "0        -39.623451       260.649954      -115.085767       -81.954942  \n",
       "9       -149.481247      -535.823873       -40.431290      -470.993059  \n",
       "24       218.967561       -66.321636      -200.393450      -107.128517  \n",
       "31      -791.597461      -822.562965      -356.240988      -118.945277  \n",
       "45      -141.267965      -197.021311      -217.041629      -277.950585  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View 2:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rs4575098</th>\n",
       "      <th>rs6656401</th>\n",
       "      <th>rs2093760</th>\n",
       "      <th>rs4844610</th>\n",
       "      <th>rs4663105</th>\n",
       "      <th>rs6733839</th>\n",
       "      <th>rs10933431</th>\n",
       "      <th>rs35349669</th>\n",
       "      <th>rs6448453</th>\n",
       "      <th>rs190982</th>\n",
       "      <th>...</th>\n",
       "      <th>rs28394864</th>\n",
       "      <th>rs111278892</th>\n",
       "      <th>rs3752246</th>\n",
       "      <th>rs4147929</th>\n",
       "      <th>rs41289512</th>\n",
       "      <th>rs3865444</th>\n",
       "      <th>rs6024870</th>\n",
       "      <th>rs6014724</th>\n",
       "      <th>rs7274581</th>\n",
       "      <th>rs429358</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    rs4575098  rs6656401  rs2093760  rs4844610  rs4663105  rs6733839  \\\n",
       "0           0          0          0          0          1          1   \n",
       "9           1          0          0          0          0          0   \n",
       "24          0          0          0          0          1          0   \n",
       "31          0          1          1          1          0          0   \n",
       "45          0          0          0          0          1          1   \n",
       "\n",
       "    rs10933431  rs35349669  rs6448453  rs190982  ...  rs28394864  rs111278892  \\\n",
       "0            1           0          0         1  ...           0            1   \n",
       "9            0           1          0         0  ...           1            0   \n",
       "24           1           0          0         1  ...           2            0   \n",
       "31           0           2          1         1  ...           1            0   \n",
       "45           0           1          0         0  ...           1            0   \n",
       "\n",
       "    rs3752246  rs4147929  rs41289512  rs3865444  rs6024870  rs6014724  \\\n",
       "0           1          1           0          0          0          0   \n",
       "9           1          1           0          1          0          0   \n",
       "24          0          0           0          1          0          0   \n",
       "31          0          0           1          1          0          0   \n",
       "45          0          0           1          1          0          0   \n",
       "\n",
       "    rs7274581  rs429358  \n",
       "0           0         1  \n",
       "9           0         0  \n",
       "24          0         0  \n",
       "31          0         2  \n",
       "45          0         1  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"View 1:\")\n",
    "display(view_1.head())\n",
    "print(\"View 2:\")\n",
    "display(view_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8102d",
   "metadata": {},
   "source": [
    "### Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abc22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a gpu exists, torch.device should be 'gpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('gpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "\n",
    "# the size of the new space learned by the model (number of the new features)\n",
    "outdim_size = 145\n",
    "\n",
    "# size of the input for view 1 and view 2\n",
    "input_shape1 = 145 # view_1.shape[1]\n",
    "input_shape2 = 54  # view_2.shape[2]\n",
    "\n",
    "# number of layers with nodes in each one\n",
    "# this apparently can be different for each network, some experimentation is needed!\n",
    "layer_sizes1 = [256, 1024, 1024, outdim_size]\n",
    "layer_sizes2 = [256, 1024, 1024, outdim_size]\n",
    "# layer_sizes1 = [64, 128, outdim_size]\n",
    "# layer_sizes2 = [64, 128, outdim_size]\n",
    "# the parameters for training the network\n",
    "learning_rate = 1e-4\n",
    "epoch_num = 500\n",
    "epoch_log_freq = 50\n",
    "batch_size = 1000\n",
    "\n",
    "# the path to save the final learned features, as DCCA-o-d.\n",
    "save_to = './DATA/ADNI_DCCA_features_'+str(outdim_size)+'_'+str(len(layer_sizes1)-1)+'.pkl'\n",
    "\n",
    "# the regularization parameter of the network\n",
    "# seems necessary to avoid the gradient exploding especially when non-saturating activations are used\n",
    "reg_par = 1e-3\n",
    "\n",
    "# specifies if all the singular values should get used to calculate the correlation or just the top \n",
    "# outdim_size ones\n",
    "# if one option does not work for a network or dataset, try the other one\n",
    "use_all_singular_values = False\n",
    "\n",
    "# if a linear CCA should get applied on the learned features extracted from the networks\n",
    "# it does not affect the performance on noisy MNIST significantly\n",
    "apply_linear_cca = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32dded",
   "metadata": {},
   "source": [
    "###  Training the DCCA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4ebdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas dataframe to numpy arrays for pytorch:\n",
    "view_1_n = view_1.to_numpy()\n",
    "view_2_n = view_2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e14d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1302, 145) <class 'numpy.ndarray'> float64\n",
      "(1302, 54) <class 'numpy.ndarray'> float64\n",
      "torch.Size([1302, 145]) <class 'torch.Tensor'>\n",
      "torch.Size([1302, 54]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Scramble the datapoints for randomness:\n",
    "indices = np.arange(view_1_n.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "view_1_n = view_1_n[indices]\n",
    "view_2_n = view_2_n[indices].astype(np.float64) # DeepCCA MLP requires double type\n",
    "\n",
    "print(view_1_n.shape, type(view_1_n), view_1_n.dtype)\n",
    "print(view_2_n.shape, type(view_2_n), view_2_n.dtype)\n",
    "\n",
    "view_1_t = torch.from_numpy(view_1_n)\n",
    "print(view_1_t.shape, type(view_1_t))\n",
    "view_2_t = torch.from_numpy(view_2_n)\n",
    "print(view_2_t.shape, type(view_2_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f516e24f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2022-03-03 00:55:29,695 ] - DataParallel(\n",
      "  (module): DeepCCA(\n",
      "    (model1): MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=145, out_features=256, bias=True)\n",
      "          (1): Sigmoid()\n",
      "          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): Sigmoid()\n",
      "          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (1): Sigmoid()\n",
      "          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "          (1): Linear(in_features=1024, out_features=145, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (model2): MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=54, out_features=256, bias=True)\n",
      "          (1): Sigmoid()\n",
      "          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): Sigmoid()\n",
      "          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (1): Sigmoid()\n",
      "          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "          (1): Linear(in_features=1024, out_features=145, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2022-03-03 00:55:29,696 ] - RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    momentum: 0\n",
      "    weight_decay: 0.001\n",
      ")\n",
      "[ INFO : 2022-03-03 00:55:29,991 ] - Epoch 1/500 - time: 0.29 - training_loss: -42.0028 - val_loss: -4.6693\n",
      "[ INFO : 2022-03-03 00:55:47,865 ] - Epoch 51/500 - time: 0.31 - training_loss: -114.1431 - val_loss: -85.8570\n",
      "[ INFO : 2022-03-03 00:56:03,993 ] - Epoch 101/500 - time: 0.30 - training_loss: -124.4661 - val_loss: -104.9816\n",
      "[ INFO : 2022-03-03 00:56:19,466 ] - Epoch 151/500 - time: 0.31 - training_loss: -129.2818 - val_loss: -107.3210\n",
      "[ INFO : 2022-03-03 00:56:34,829 ] - Epoch 201/500 - time: 0.30 - training_loss: -132.1335 - val_loss: -108.9086\n",
      "[ INFO : 2022-03-03 00:56:50,450 ] - Epoch 251/500 - time: 0.30 - training_loss: -134.0448 - val_loss: -110.1003\n",
      "[ INFO : 2022-03-03 00:57:05,989 ] - Epoch 301/500 - time: 0.30 - training_loss: -135.4220 - val_loss: -110.9194\n",
      "[ INFO : 2022-03-03 00:57:21,650 ] - Epoch 351: val_loss did not improve from -111.6669\n",
      "[ INFO : 2022-03-03 00:57:21,651 ] - Epoch 351/500 - time: 0.33 - training_loss: -136.4708 - val_loss: -111.6531\n",
      "[ INFO : 2022-03-03 00:57:37,580 ] - Epoch 401/500 - time: 0.31 - training_loss: -137.2945 - val_loss: -112.2397\n",
      "[ INFO : 2022-03-03 00:57:53,803 ] - Epoch 451/500 - time: 0.32 - training_loss: -137.9644 - val_loss: -112.7685\n",
      "[ INFO : 2022-03-03 00:58:09,536 ] - loss on validation data: -113.1041\n",
      "[ INFO : 2022-03-03 00:58:09,584 ] - loss on test data: -122.8617\n"
     ]
    }
   ],
   "source": [
    "data1 = view_1_t\n",
    "data2 = view_2_t\n",
    "\n",
    "model = DeepCCA(layer_sizes1, layer_sizes2, input_shape1,\n",
    "                input_shape2, outdim_size, use_all_singular_values, device=device).double()\n",
    "l_cca = None\n",
    "if apply_linear_cca:\n",
    "    l_cca = linear_cca()\n",
    "    \n",
    "    \n",
    "solver = Solver(model, l_cca, outdim_size, epoch_num, batch_size,\n",
    "                learning_rate, reg_par, device=device, epoch_log_freq=epoch_log_freq, log=False)\n",
    "s_1, s_2 = data1.shape[0], data2.shape[0]\n",
    "\n",
    "# Split the dataset into training, validation and testing (75%-15%-10%):\n",
    "train1, train2 = data1[0:int(s_1 * 0.75)], data2[0:int(s_2 * 0.75)]\n",
    "val1, val2 = data1[int(s_1 * 0.75):int(s_1 * 0.9)], data2[int(s_2 * 0.75):int(s_2 * 0.9)]\n",
    "test1, test2 = data1[int(s_1 * 0.9):], data2[int(s_2 * 0.9):]\n",
    "\n",
    "solver.fit(train1, train2, val1, val2, test1, test2, checkpoint=None)\n",
    "training_losses, val_losses = solver.get_losses()\n",
    "# TODO: Save linear_cca model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e11b95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-140.642  -89.145]\n"
     ]
    }
   ],
   "source": [
    "set_size = [0, \n",
    "            train1.size(0), \n",
    "            train1.size(0) + val1.size(0), \n",
    "            train1.size(0) + val1.size(0) + test1.size(0)]\n",
    "\n",
    "losses, outputs = solver._get_outputs(data1, data2)\n",
    "losses = np.round(losses,3)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b361e6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1302, 145)\n",
      "<class 'numpy.ndarray'>\n",
      "(1302, 145)\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs[0]))\n",
    "print(outputs[0].shape)\n",
    "print(type(outputs[1]))\n",
    "print(outputs[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0e965-1757-4085-857b-ffe541adb8db",
   "metadata": {},
   "source": [
    "### Plotting the Losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f371c9de-f4b6-40db-9545-493b85b720be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(epoch_num)\n",
    "plt.plot(x, training_losses, color='blue', label=\"Training Loss\")\n",
    "plt.plot(x, val_losses, color='orange', label=\"Validation Loss\")\n",
    "plt.title(\"Losses for the training of the model\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e3e90",
   "metadata": {},
   "source": [
    "### Saving the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23a5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving new features in a gzip pickled file specified by save_to\n",
    "with open(save_to, 'wb') as f:\n",
    "    pickle.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f517c8f",
   "metadata": {},
   "source": [
    "### Loading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bacfa2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = torch.load('checkpoint.model')\n",
    "# solver.model.load_state_dict(d)\n",
    "# solver.model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d09c7-c1b5-4c68-bf8f-d1d96b320925",
   "metadata": {},
   "source": [
    "## Testing the Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5022e0f",
   "metadata": {},
   "source": [
    "### Testing the Correlation between inputs and outputs of the deep Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019cb979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCA on input data:\n",
      "-0.42426306714130085\n",
      "CCA on output data:\n",
      "0.27795515365866535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "print(\"CCA on input data:\")\n",
    "X = data1\n",
    "Y = data2\n",
    "cca = CCA(n_components=50)\n",
    "cca.fit(X, Y)\n",
    "X_c, Y_c = cca.transform(X, Y)\n",
    "print(cca.score(X, Y))\n",
    "\n",
    "print(\"CCA on output data:\")\n",
    "X = outputs[0]\n",
    "Y = outputs[1]\n",
    "cca = CCA(n_components=50,max_iter=10000)\n",
    "cca.fit(X, Y)\n",
    "X_c, Y_c = cca.transform(X, Y)\n",
    "# The best possible score is 1.0 and it can be negative \n",
    "# (because the model can be arbitrarily worse)\n",
    "print(cca.score(X, Y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16aa69",
   "metadata": {},
   "source": [
    "### (Imaging) Training and testing of SVM with linear kernel on the view 1 with new features vs old features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a074bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for untrained data: {'C': 10}\n",
      "Untrained Accuracy:  51.381\n",
      "Best Parameters for trained data: {'C': 0.001}\n",
      "Trained Accuracy:    48.849\n"
     ]
    }
   ],
   "source": [
    "s = svm.LinearSVC(dual=False)\n",
    "X , Y = view_1, unique[\"Diagnosis_nearest_2.0_cat\"]\n",
    "parameters = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "clf = GridSearchCV(s, parameters, n_jobs=-1, cv=10)\n",
    "clf.fit(X,Y)\n",
    "print(\"Best Parameters for untrained data:\",clf.best_params_)\n",
    "acc_untrained = clf.best_score_\n",
    "print(\"Untrained Accuracy: \", round(np.mean(acc_untrained)*100,3))\n",
    "\n",
    "s = svm.LinearSVC(dual=False)\n",
    "X , Y = outputs[0], unique[\"Diagnosis_nearest_2.0_cat\"]\n",
    "parameters = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "clf = GridSearchCV(s, parameters, n_jobs=-1, cv=10)\n",
    "clf.fit(X,Y)\n",
    "print(\"Best Parameters for trained data:\",clf.best_params_)\n",
    "acc_trained = clf.best_score_\n",
    "print(\"Trained Accuracy:   \", round(np.mean(acc_trained)*100,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84180f5",
   "metadata": {},
   "source": [
    "### (Genetic) Training and testing of SVM with linear kernel on the view 2 with new features vs old features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "678964ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for untrained data: {'C': 0.0001}\n",
      "Untrained Accuracy:  48.08\n",
      "Best Parameters for trained data: {'C': 0.01}\n",
      "Trained Accuracy:    46.929\n"
     ]
    }
   ],
   "source": [
    "s = svm.LinearSVC(dual=False)\n",
    "X , Y = view_2, unique[\"Diagnosis_nearest_2.0_cat\"]\n",
    "parameters = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "clf = GridSearchCV(s, parameters, n_jobs=-1, cv=10)\n",
    "clf.fit(X,Y)\n",
    "print(\"Best Parameters for untrained data:\",clf.best_params_)\n",
    "acc_untrained = clf.best_score_\n",
    "print(\"Untrained Accuracy: \", round(np.mean(acc_untrained)*100,3))\n",
    "\n",
    "s = svm.LinearSVC(dual=False)\n",
    "X , Y = outputs[1], unique[\"Diagnosis_nearest_2.0_cat\"]\n",
    "parameters = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "clf = GridSearchCV(s, parameters, n_jobs=-1, cv=10)\n",
    "clf.fit(X,Y)\n",
    "print(\"Best Parameters for trained data:\",clf.best_params_)\n",
    "acc_trained = clf.best_score_\n",
    "print(\"Trained Accuracy:   \", round(np.mean(acc_trained)*100,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017fe62",
   "metadata": {},
   "source": [
    "### (Imaging + Genetic) Training and testing of SVM with linear kernel on both views with new features vs old features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "461324ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1302, 145)\n",
      "(1302, 145)\n",
      "(1302, 290)\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0].shape)\n",
    "print(outputs[1].shape)\n",
    "both = np.concatenate((outputs[0], outputs[1]), axis=1)\n",
    "print(both.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12fd3caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for untrained data: {'C': 0.1}\n",
      "Untrained Accuracy:  52.988\n",
      "Best Parameters for trained data: {'C': 0.001}\n",
      "Trained Accuracy:    48.695\n"
     ]
    }
   ],
   "source": [
    "c = list(unique.columns)\n",
    "MRI_columns = c[c.index(\"MUSE_Volume_4\"):c.index(\"MUSE_Volume_207\")+1]\n",
    "genetic_columns = c[c.index(\"rs4575098\"):c.index(\"rs429358\")+1]\n",
    "columns_of_interest = []\n",
    "columns_of_interest += MRI_columns + genetic_columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s = svm.LinearSVC(dual=False)\n",
    "X , Y = unique[columns_of_interest] , unique[\"Diagnosis_nearest_2.0_cat\"]\n",
    "parameters = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "clf = GridSearchCV(s, parameters, n_jobs=-1, cv=10)\n",
    "clf.fit(X,Y)\n",
    "print(\"Best Parameters for untrained data:\",clf.best_params_)\n",
    "acc_untrained = clf.best_score_\n",
    "print(\"Untrained Accuracy: \", round(np.mean(acc_untrained)*100,3))\n",
    "\n",
    "s = svm.LinearSVC(dual=False)\n",
    "X , Y = both, unique[\"Diagnosis_nearest_2.0_cat\"]\n",
    "parameters = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "clf = GridSearchCV(s, parameters, n_jobs=-1, cv=10)\n",
    "clf.fit(X,Y)\n",
    "print(\"Best Parameters for trained data:\",clf.best_params_)\n",
    "acc_trained = clf.best_score_\n",
    "print(\"Trained Accuracy:   \", round(np.mean(acc_trained)*100,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
